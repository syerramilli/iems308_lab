{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.1: Solutions\n",
    "\n",
    "## TA: Suraj Yerramilli\n",
    "\n",
    "## Date: February 18th, 2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = \"Each year, millions of Americans walk out of a doctor's office with a misdiagnosis. Physicians try to be systematic when identifying illness and disease, but bias creeps in. Alternatives are overlooked.\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "Implement `sent_tokenize` and `word_tokenize` using regex. Demonstrate them on the `text` variable.\n",
    "\n",
    "### sent_tokenize \n",
    "\n",
    "The simplest attempt is to split by punctuation and not capture them. Note that you need to also include a whitespace following these punctuations.\n",
    "\n",
    "Also, you must have noticed by now that `re.split` returns an empty character at the end of the list. You can get rid of this by using the `filter` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution with regex split will not include punctuation\n",
    "# need to include white space in splitting criteria\n",
    "# potential failures - last abbreviation eg: U.S.A. - sentence split after A\n",
    "def sent_tokenize_split(text):\n",
    "    sentences =  re.split(r'[\\.\\?!]\\s*',text)\n",
    "    # solution to remove the empty character at the end of the list\n",
    "    return list(filter(None,sentences))\n",
    "\n",
    "sent_tokenize_split(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to capture the punctuation along with the sentence (which would be ideal), include only the punctuation groups in a a group. `re.split` splits the string by the given regex and also returns the captured patterns. You just need to combine them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_tokenize_split2(text):\n",
    "    l =  re.split(r'(\\.|\\?|!)\\s*',text)\n",
    "    # solution to remove the empty character at the end of the list\n",
    "    l = list(filter(None,l))\n",
    "    \n",
    "    sentences = []\n",
    "    for i in range(0,len(l),2):\n",
    "        sentences.append(l[i]+l[i+1])\n",
    "        \n",
    "    return sentences\n",
    "\n",
    "sent_tokenize_split2(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem with the above two solutions is sentences with abbreviations. This, however, is not a deal-breaker because the abbreviations are usually replaced with continuous letters of text. \n",
    "\n",
    "There is a regex solution to deal with abbreviations and it involves using \"look behind\" and \"look-ahead\" groups. \n",
    "\n",
    "- (?<!...) \"look-behind\": groups before the actual pattern\n",
    "- (?<=...) \"look-ahead\": groups after the actual pattern\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Mr. Smith is planning to visit New York on his U.S.A. trip. He holds a B.Tech from Oxford.\"\n",
    "re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s+',sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Each', 'year', ',', 'millions', 'of', 'Americans', 'walk', 'out', 'of', 'a', 'doctor', \"'s\", 'office', 'with', 'a', 'misdiagnosis', '.', 'Physicians', 'try', 'to', 'be', 'systematic', 'when', 'identifying', 'illness', 'and', 'disease', ',', 'but', 'bias', 'creeps', 'in', '.', 'Alternatives', 'are', 'overlooked', '.']\n"
     ]
    }
   ],
   "source": [
    "def word_tokenize_regex(text):\n",
    "    # need to deal with abbreviations ahead of calling this function\n",
    "    return re.findall(r'\\w+|[;\\.,!\\?\\:]|\\'\\w+',text)\n",
    "\n",
    "print(word_tokenize_regex(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Each', 'year', ',', 'millions', 'of', 'Americans', 'walk', 'out', 'of', 'a', 'doctor', \"'s\", 'office', 'with', 'a', 'misdiagnosis', '.', 'Physicians', 'try', 'to', 'be', 'systematic', 'when', 'identifying', 'illness', 'and', 'disease', ',', 'but', 'bias', 'creeps', 'in', '.', 'Alternatives', 'are', 'overlooked', '.']\n"
     ]
    }
   ],
   "source": [
    "# output from word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Given a sentence, how would you correctly lemmatize words according to their parts of speech?\n",
    "\n",
    "The idea is to apply POS tagging first and then lemmatize accordingly. The first letter of the tag will identify the POS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tags:\n",
      "[('The', 'DT'), ('striped', 'JJ'), ('bats', 'NNS'), ('are', 'VBP'), ('hanging', 'VBG'), ('on', 'IN'), ('their', 'PRP$'), ('feet', 'NNS'), ('for', 'IN'), ('best', 'JJS'), ('.', '.')]\n",
      "\n",
      "Lemmatized sentence:\n",
      "['The', 'striped', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'best', '.']\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE GOES HERE\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def lemmatize_pos(lemmatizer,tokens):\n",
    "    # need to lemmatize based on parts of speech:\n",
    "    # the first character of the tag usually \n",
    "    # indicates the parts of speech\n",
    "    out = []\n",
    "    for token,tag in pos_tag(tokens):\n",
    "        if tag[0] == 'N':\n",
    "            # noun\n",
    "            tmp = lemmatizer.lemmatize(token,\"n\")\n",
    "        elif tag[0] == \"V\":\n",
    "            # verb\n",
    "            tmp = lemmatizer.lemmatize(token,\"v\")\n",
    "        elif tag[0] == \"J\":\n",
    "            # adjective\n",
    "            tmp = lemmatizer.lemmatize(token,\"a\")\n",
    "        elif tag[0] == \"R\":\n",
    "            # adverb\n",
    "            tmp = lemmatizer.lemmatize(token,\"r\")\n",
    "        else:\n",
    "            tmp = token\n",
    "            \n",
    "        out.append(tmp)\n",
    "        \n",
    "    return out\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sent = \"The striped bats are hanging on their feet for best.\"\n",
    "tokens = word_tokenize(sent)\n",
    "\n",
    "print(\"POS tags:\")\n",
    "print(pos_tag(tokens))\n",
    "print(\"\\nLemmatized sentence:\")\n",
    "print(lemmatize_pos(lemmatizer,tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tags:\n",
      "[('The', 'DT'), ('bats', 'NNS'), ('with', 'IN'), ('stripes', 'NNS'), ('are', 'VBP'), ('better', 'JJR'), ('off', 'RP'), ('hanging', 'VBG'), ('on', 'IN'), ('their', 'PRP$'), ('feet', 'NNS'), ('.', '.')]\n",
      "\n",
      "Lemmatized sentence:\n",
      "['The', 'bat', 'with', 'stripe', 'be', 'good', 'off', 'hang', 'on', 'their', 'foot', '.']\n"
     ]
    }
   ],
   "source": [
    "# another example\n",
    "sent = \"The bats with stripes are better off hanging on their feet.\"\n",
    "tokens = word_tokenize(sent)\n",
    "\n",
    "print(\"POS tags:\")\n",
    "print(pos_tag(tokens))\n",
    "print(\"\\nLemmatized sentence:\")\n",
    "print(lemmatize_pos(lemmatizer,tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
