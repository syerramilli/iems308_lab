{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.2: Sentiment Analysis\n",
    "\n",
    "## TA: Suraj Yerramilli\n",
    "\n",
    "## Date: February 18th, 2019\n",
    "\n",
    "In this lab, we will peform sentiment analysis on the IMDB reviews dataset [1] using Bag-of-Words(BoW) models and Naive-Bayes classifiers. The IMDB reviews dataset is a collection of 50,000 movie reviews from IMDB with evenly balanced number of positive (score $\\geq$ 7 out of 10) and negative (score $\\leq$ 4 out of 10) reviews. Therefore, randomly classifying the data will lead to 50% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the data\n",
    "\n",
    "The dataset has two folders, train and test, which contain 25000 reviews each, with balanced number of positive and negative reviews. The positive reviews are located in the `pos` subdirectory while the negative reviews are located in the `neg` subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training set\n",
    "reviews_train = []\n",
    "y_train = []\n",
    "\n",
    "# reading positive reviews\n",
    "for file in os.listdir('../data/aclImdb/train/pos/'):\n",
    "    reviews_train.append(open(os.path.join('../data/aclImdb/train/pos',file),'rb').read().decode('utf-8'))\n",
    "    y_train.append(1)\n",
    "    \n",
    "# reading negative reviews\n",
    "for file in os.listdir('../data/aclImdb/train/neg/'):\n",
    "    reviews_train.append(open(os.path.join('../data/aclImdb/train/neg',file),'rb').read().decode('utf-8'))\n",
    "    y_train.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 5 positive and negative reviews\n",
    "print(\"First 5 positive Reviews:\")\n",
    "print(\"********\")\n",
    "for i in range(5):\n",
    "    print(reviews_train[i])\n",
    "    print(\"********\")\n",
    "\n",
    "print(\"****************\")\n",
    "print()\n",
    "print(\"First 5 negative Reviews:\")\n",
    "print(\"********\")\n",
    "for i in range(5):\n",
    "    print(reviews_train[12500+i])\n",
    "    print(\"********\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Load the test set. Denote the text and labels by `reviews_test` and `y_test` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR CODE GOES HERE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize text\n",
    "\n",
    "We will use a simple regex-based tokenizer to convert each review into a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define regex tokenizer\n",
    "tokenizer = RegexpTokenizer(\"\\w+\")\n",
    "\n",
    "# tokenize text - remember to convert text to lower case\n",
    "tokens_train = [tokenizer.tokenize(review.lower()) for review in reviews_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Tokenize the test data. Denote the tokenized text by `tokens_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR CODE GOES HERE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using  document-term matrix\n",
    "\n",
    "We will first generate the document-term matrix and use the counts of the words as features for the classification algorithm. Note that you need to first learn vocabulary from the training set, and then use the fitted object to generate document-term matrix for the test set.\n",
    "\n",
    "We will be only counting unigrams, i.e. single words. We will be removing words which occur in less than 10 reviews in the training set. We, however, won't be removing stopwords for now.\n",
    "\n",
    "The `CountVectorizer` class is used for this purpose. The code below does the following:\n",
    "\n",
    "1. Intialize a `CountVectorizer` object with the necessary arguments/\n",
    "2. Learn vocabulary from the training set, and obtain the document-term matrix for the training set (using the `.fit_transform` method)\n",
    "3. Obtain the document-term matrix for the test set (using the `.transform` method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "tf = CountVectorizer(tokenizer=identity,preprocessor=identity,\n",
    "                             ngram_range=(1, 1),stop_words=None,min_df=10)\n",
    "X_train = tf.fit_transform(tokens_train)\n",
    "X_test = tf.transform(tokens_test) # applying transform to test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of transform is a sparse matrix (**why sparse?**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train a Binomial Naive-Bayes classifier (there are only two classes) on the data. Naive-Bayes classifiers are a family of simple probablistic classifiers. They scale linearly with the data and so, are very fast to train. Despite their simplicity, they are found to work quite well, particularly for document classification. Hence, they offer a useful baseline performace for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and test accuracies are calculated below. You should get a test accuracy of 0.81.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training score\n",
    "y_train_pred = clf.predict(X_train)\n",
    "print(\"Training accuracy: {}\".format(accuracy_score(y_train,y_train_pred)))\n",
    "\n",
    "# Testing score\n",
    "y_test_pred = clf.predict(X_test)\n",
    "print(\"Test accuracy: {}\".format(accuracy_score(y_test,y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Instead of just using word counts, we could weight them with their respective document frequencies. So, words appearing very frequently will have low weights. \n",
    "\n",
    "Use`TfidfVectorizer` (takes the same arguments) to generate this weighted document-term matrix and build a Naive-Bayes classifier. Is there any change in performance? If yes, why would this weighting affect the performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "We haven't removed stop words yet. Repeat the above two classification tasks (CountVectorizer and TfidfVectorizer) with removing stop words from vocabulary. You need to pass a list of stop words to the `stop_words` argument. How does removing stop words impact classification performance in either case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Exercises (won't be discussed in the lab)\n",
    "\n",
    "1. We have used Naive-Bayes for classification. As previously mentioned, this provides a useful benchmark for classification performance. Try using other classifiers such as logistic regression and linear support vector machines. \n",
    "2. The \"terms\" in the document-term matrix are unigrams. How does allowing bigrams impact classification performance? This is controlled by the `ngram_range` argument. Note that you may need to remove stopwords.\n",
    "3. How does stemming and lemmatization impact classification performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1]  Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., & Potts, C. (2011, June). Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies-volume 1 (pp. 142-150). Association for Computational Linguistics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
