{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.1: Regex and NLTK\n",
    "\n",
    "## TA: Suraj Yerramilli\n",
    "\n",
    "## Date: February 18th, 2019\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular expressions\n",
    "\n",
    "In Python, the `re` module provides regular expression support. To define a regular expression use either a string preceeded by the literal r (called a raw string) or use the `re.compile` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "## Two ways of declaring a regex\n",
    "pat = r'[A-Z][a-z]+'\n",
    "pat = re.compile('[A-Z][a-z]+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the above expressions capture patterns beginning with an uppercase letter followed by at least one or more lower case letters. \n",
    "\n",
    "### Searching\n",
    "\n",
    "A simple search expression is as follows:\n",
    "\n",
    "```\n",
    "match = re.search(pat,s)\n",
    "```\n",
    "\n",
    "The `re.search` method takes a regex pattern and a string and searchs for the *first occurence* of the pattern within the string. If the search is successful, a Match object is returned. Otherwise, `None` is returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"Bob has assigned the task to Alice.\"\n",
    "s2 = \"A cat has nine lives\"\n",
    "\n",
    "match1 = re.search(pat,s1)\n",
    "match2 = re.search(pat,s2)\n",
    "print(\"Returned value: {}\".format(match1))\n",
    "print(\"Returned value: {}\".format(match2))\n",
    "\n",
    "print(\"Matched value in string: {}\".format(match1.group()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find all the matches use the `re.findall` function. This resturns a list of all matched patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(pat,s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Substituting\n",
    "\n",
    "The `re.sub` function takes a pattern to be replaced, the replacement string, and the string within which the pattern is to be replaced. If no pattern is found, the string is return unchanged. The following code replaces all emails with \"emailToken\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Send us an email to either abc123@gmail.com or xyz@outlook.com\"\n",
    "re.sub(r'[A-Za-z0-9\\.-_]+@[A-Za-z\\.]+','emailToken',s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Tim Cook is the CEO of Apple. Apple was founded by Steve Jobs and Steve Wozniak in the late 70s.\"\n",
    "match = re.match(r'([A-Z]\\w+) ([A-Z]\\w+)',s)\n",
    "\n",
    "print(\"Entire match: {}\".format(match.group(0)))\n",
    "print(\"Group 1 (First Name) match: {}\".format(match.group(1)))\n",
    "print(\"Group 2 (Last Name) match: {}\".format(match.group(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all names\n",
    "re.findall(r'([A-Z]\\w+) ([A-Z]\\w+)',s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting only last names\n",
    "re.findall(r'[A-Z]\\w+ ([A-Z]\\w+)',s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract service name and domain\n",
    "s = \"abc123@gmail.com xyz@outlook.com werz@organization.org\"\n",
    "re.findall(r'@(\\w+)\\.(\\w+)',s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting\n",
    "\n",
    "The `re.split` function is used to split string by occurences of pattern. If there are groups in the pattern, then the text of all the groups are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example split by whitespace (spaces, newlines tabs)\n",
    "s = \"Roses are red,\\n Violets are blue\"\n",
    "print(\"Original String:\")\n",
    "print(s)\n",
    "print(\"\\nSplitted string:\")\n",
    "print(re.split(r'\\s',s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Why did that the split output return a blank character? How would you correct this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split with groups in pattern\n",
    "s = \"Send us an email to either abc123@gmail.com or xyz@outlook.com\"\n",
    "re.split(r'[A-Za-z0-9\\.-_]+@([A-Za-z\\.]+)',s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The NLTK library\n",
    "\n",
    "The NLTK package provides various NLP algorithms such as tokenization, stemming, lemmatization, parts of speech tagging, etc.\n",
    "\n",
    "**Note**: You will be needing additional downloads if your using nltk for the first time. Run the following lines from the iPython console.\n",
    "\n",
    "```\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "```\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "The first step in NLP is breaking down text into units - tokens. \n",
    "\n",
    "`sent_tokenize` breaks texts into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize,RegexpTokenizer\n",
    "text = \"Each year, millions of Americans walk out of a doctor's office with a misdiagnosis. Physicians try to be systematic when identifying illness and disease, but bias creeps in. Alternatives are overlooked.\"\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`word_tokenize` splits text by punctutation and white spaces. The punctutations are captured. Contractions such as \"shoudn't\" are split (\"shouldn\",\"'t\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text by tokens\n",
    "tokens_text = word_tokenize(text)\n",
    "print(\"Tokens for entire text:\")\n",
    "print(tokens_text)\n",
    "\n",
    "tokens_sent1 = word_tokenize(sentences[0])\n",
    "print(\"\\nTokens for the first sentence\")\n",
    "print(tokens_sent1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RegexpTokenizer` allows you to capture tokens by regex pattern instead of splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex tokenizer\n",
    "tokenizer = RegexpTokenizer(\"\\w+\")\n",
    "tokens_sent2 = tokenizer.tokenize(sentences[0])\n",
    "print(tokens_sent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nltk.tokenize` submodule has specialized tokenizers such as `TweetTokenizer`, which is used for tokenizing tweets and special characters such as \n",
    "\n",
    "**Exercise**:\n",
    "Implement `sent_tokenize` and `word_tokenize` using regex. Demonstrate them on the `text` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging\n",
    "\n",
    "The `pos_tag` function handles parts of speech tagging. A list of POS tags is given here https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "sent = \"The cat jumped over the gate.\"\n",
    "sent_tokens = word_tokenize(sent)\n",
    "sent_tagged = pos_tag(sent_tokens)\n",
    "print(sent_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about words that have different meanings in the same sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same words appearing as parts of speech\n",
    "sent = \"They refuse to permit us to obtain the refuse permit.\"\n",
    "sent_tokens = word_tokenize(sent)\n",
    "sent_tagged = pos_tag(sent_tokens)\n",
    "print(sent_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words refuse and permit are verbs in their first occurrence while they are nouns in their occurrence. The POS tagger is and should be able to distinguish the two contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "There are several stemming algorithms provided by NLTK.  There are a few differences amongst these stemmers. In particular, some of their stemming rules are different. The `PorterStemmer` is a very widely used one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "print(\"Original sentence: {}\".format(tokens_sent2))\n",
    "stemmer = PorterStemmer()\n",
    "print([stemmer.stem(token) for token in tokens_sent2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the stemmer converted the text to lowercase. This is the default behavior. Which words have been stemmed? \n",
    "\n",
    "### Lemmatization\n",
    "\n",
    "The NLTK lemmatization method is based on WordNet. Lemmatization is more sophisticated than stemming and has an advantage that we get actual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sent = \"Why are there so many monkeys in this area?\"\n",
    "\n",
    "print([lemmatizer.lemmatize(token) for token in word_tokenize(sent)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the lemma for \"are\" is \"be\". Why wasn't this shown in the output? This is because the lemmas are different depending on the parts of speech. The `.lemmatize` method accepts two arguments - the word and its part of speech. If the latter is not supplied, the word is taken to be a noun by default.\n",
    "\n",
    "Parsing the above text as verbs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([lemmatizer.lemmatize(token,\"v\") for token in word_tokenize(sent)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two examples demonstrate stemming versus lemmatization. They also show the difference of lemmas for nouns and verbs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"flying\"\n",
    "print(\"Stemmed Word:\",stemmer.stem(word))\n",
    "print(\"Lemmatized Word (Noun):\",lemmatizer.lemmatize(word,\"n\"))\n",
    "print(\"Lemmatized Word (Verb):\",lemmatizer.lemmatize(word,\"v\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"stripes\"\n",
    "print(\"Stemmed Word:\",stemmer.stem(word))\n",
    "print(\"Lemmatized Word (Noun):\",lemmatizer.lemmatize(word,\"n\"))\n",
    "print(\"Lemmatized Word (Verb):\",lemmatizer.lemmatize(word,\"v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization applies to other parts of speech as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"better\"\n",
    "print(\"Stemmed Word:\",stemmer.stem(word))\n",
    "print(\"Lemmatized Word (Noun):\",lemmatizer.lemmatize(word,\"n\"))\n",
    "print(\"Lemmatized Word (Verb):\",lemmatizer.lemmatize(word,\"v\"))\n",
    "print(\"Lemmatized Word (Adjective):\",lemmatizer.lemmatize(word,'a'))\n",
    "print(\"Lemmatized Word (Adverb):\",lemmatizer.lemmatize(word,'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Given a sentence, how would you correctly lemmatize words according to their parts of speech?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words\n",
    "\n",
    "NLTK provides list of universally agreed stopwords for various languages. To extract stopwords in English:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=sorted(set(stopwords.words(\"english\")))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove stop words, you need to compare each token with the ones on the stop word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of removing stop words\n",
    "tokenized_sent = word_tokenize(\"This is not the way to go about the problem.\")\n",
    "filtered_sent = [word for word in tokenized_sent if word not in stop_words]\n",
    "\n",
    "print(\"Original sentence: {}\".format(tokenized_sent))\n",
    "print(\"Filtered sentence: {}\".format(filtered_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: \"This\" is one of the stop-words. Why wasn't it filtered out? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
